{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DATE: 8/29/2020\n",
    "AUTHOR: DANIEL WU\n",
    "PURPOSE: Execute the webscraping algorithm for factiva reports \n",
    "LAST UPDATE:\n",
    "\"\"\"\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Import Packages\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from getpass import getpass\n",
    "import json\n",
    "\n",
    "import import_ipynb\n",
    "import importlib\n",
    "\n",
    "def pause():\n",
    "    programPause = input(\"Press the <ENTER> key to continue or type any letter to break loop\")\n",
    "    return programPause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USC Password: ········\n"
     ]
    }
   ],
   "source": [
    "# Webscraping Parameters\n",
    "path_to_chromedriver = '/usr/local/bin/chromedriver'\n",
    "usclib_url = 'https://libraries.usc.edu/'\n",
    "usrnm = 'ddwu'\n",
    "pw = getpass('USC Password: ')\n",
    "\n",
    "download_folder = '/Users//user//Desktop//Jupyter Notebook//Factiva Scraping Test//WebScrapingAlgo//downloaded_articles'\n",
    "articles_folder =  '/Users//user//Desktop//Jupyter Notebook//Factiva Scraping Test//WebScrapingAlgo//downloaded_articles'\n",
    "working_dir = '/Users/user/Desktop/Jupyter Notebook/Factiva Scraping Test/WebScrapingAlgo'\n",
    "os.chdir(working_dir)\n",
    "factiva_query = '''(“equity” or “stock” or “share”) and (“offer” or “offering” or “issue” or “issuance” or “8K”)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Excel Spreadsheet\n",
    "co_df = pd.read_excel('Bank_scraped.xlsx') \n",
    "co_df['conm'] = co_df['conm'].str.lower().str.split(',').str[0]\n",
    "back_30 = datetime.timedelta(days=30)\n",
    "next_3 = datetime.timedelta(days=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Chrome Session\n",
    "driver = webdriver.Chrome(executable_path = path_to_chromedriver)\n",
    "driver.get(usclib_url)\n",
    "driver.implicitly_wait(5) #seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from procedural_commands.ipynb\n",
      "importing Jupyter notebook from factiva_word_storage.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Import Procedural Modules\n",
    "import procedural_commands as proc\n",
    "importlib.reload(proc) \n",
    "import procedural_commands as proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Log onto the website\n",
    "proc.log_onto_factiva(driver, usrnm, pw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eventually create a triple nested dictionary for JSON File\n",
    "\n",
    "# UNCOMMENT IF YOU REALLY WANT TO ERASE PROGRESS\n",
    "\n",
    "# json_file = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manually clear query before proceeding.\n",
      "Press the <ENTER> key to continue or type any letter to break loop\n",
      "579\n",
      "found wells fargo & company\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "procedural_commands.ipynb:283: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"        prefixed = [filename for filename in os.listdir(working_dir) if filename.startswith(\\\"factiva_dl\\\")]                                        \\n\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "606\n",
      "found toronto-dominion bank\n",
      "622\n",
      "found canadian imperial bank of commerce\n",
      "688\n",
      "found wells fargo & company\n"
     ]
    }
   ],
   "source": [
    "# Pick up where last left off. Select last index here (from spreadsheet):\n",
    "index_num = 1\n",
    "start_index = index_num - 1    # Convert to Python Indexing\n",
    "co_df = co_df.iloc[start_index:]\n",
    "\n",
    "proc.send_query(driver, factiva_query)\n",
    "\n",
    "# Enter keys and search algorithm:\n",
    "for index, row in co_df.iterrows():\n",
    "  \n",
    "    cusip = str(row['cusip'])\n",
    "        \n",
    "    excel_index = index + 1    \n",
    "        \n",
    "    #re-scrape overwritten json files\n",
    "    if row['Over_100'] != 1:\n",
    "        continue\n",
    "    if excel_index in (14, 68, 266, 270, 280, 318, 336, 381, 403, 413, 459, 531):\n",
    "        continue\n",
    "                        \n",
    "    print(excel_index)\n",
    "    json_file[excel_index] = {}      \n",
    "    \n",
    "    proc.enter_date(driver, start_index, index, row, next_3, back_30)        \n",
    "    search_result = proc.enter_company_name(driver, row)\n",
    "    \n",
    "    if search_result == False:\n",
    "        co_df['Scrape Comments'][index] = 'No search results for Company Name'\n",
    "        co_df['Articles Scraped'][index] = 0\n",
    "        continue\n",
    "        \n",
    "    proc.hit_search(driver)\n",
    "        \n",
    "    proc.get_articles(driver, working_dir, articles_folder, co_df, cusip, index, excel_index, json_file, 1)    \n",
    "    \n",
    "    try:\n",
    "        driver.find_element_by_class_name('nextItem').click()\n",
    "        time.sleep(2)\n",
    "        proc.get_articles(driver, working_dir, articles_folder, co_df, cusip, index, excel_index, json_file, 2)    \n",
    "        try:\n",
    "            driver.find_element_by_class_name('nextItem').click()\n",
    "            time.sleep(2)\n",
    "            proc.get_articles(driver, working_dir, articles_folder, co_df, cusip, index, excel_index, json_file, 3)            \n",
    "        except:\n",
    "            pass\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    time.sleep(2)\n",
    "    \n",
    "    try:\n",
    "        proc.hit_next_search(driver) \n",
    "    except:        \n",
    "        proc.hit_next_search(driver) \n",
    "        \n",
    "    time.sleep(2)\n",
    "    try:\n",
    "        proc.clear_prev_search(driver)\n",
    "    except:\n",
    "        time.sleep(5)\n",
    "        proc.clear_prev_search(driver)\n",
    "    \n",
    "#     x = pause()     \n",
    "#     if x == '': \n",
    "#         continue\n",
    "#     else: \n",
    "#         print(\"Loop Broken\")\n",
    "#         break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON CODE HERE - DO THIS BEFORE LOGGING OFF JUPYTER NOTEBOOK\n",
    "\n",
    "data_dump = open('json_file_banks_over100.txt', 'w', encoding = 'utf-8')\n",
    "json.dump(json_file, data_dump, ensure_ascii = False)\n",
    "data_dump.close()\n",
    "\n",
    "# Write to excel progress made since last run (whether fail or success)\n",
    "\n",
    "co_df.to_excel('Bank_rescraped.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([14, 68, 266, 270, 280, 318, 336, 381, 403, 413, 459, 531, 579, 606, 622, 688])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_file.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
